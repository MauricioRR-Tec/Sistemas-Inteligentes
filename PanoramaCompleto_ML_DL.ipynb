{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Panorama completo de ML y DL</center></h1>\n",
    "\n",
    "![img](https://miro.medium.com/max/1890/1*Lejtm0oGlOC5U0-J0JmGhg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aprendizaje automático (ML) está cambiando rápidamente el mundo, a partir de diversos tipos de aplicaciones e investigaciones realizadas en la industria y el mundo académico. \n",
    "\n",
    "El aprendizaje automático está afectando todos los aspectos de nuestra vida diaria. Desde asistentes de voz que utilizan PNL (Procesamiento de Lenguaje Natural) y aprendizaje automático para concertar citas, consultar nuestro calendario y reproducir música, hasta anuncios programáticos, que son tan precisos que pueden predecir lo que necesitaremos antes de siquiera pensar en ello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principales algoritmos de aprendizaje automático:\n",
    "\n",
    "1. Regresión (predicción)\n",
    "\n",
    "Usamos algoritmos de regresión para predecir valores continuos.\n",
    "\n",
    "Algoritmos de regresión:\n",
    "\n",
    "    Regresión lineal\n",
    "    Regresión polinomial\n",
    "    Regresión exponencial\n",
    "    Regresión logística\n",
    "    Regresión logarítmica\n",
    "\n",
    "2. Clasificación\n",
    "\n",
    "Usamos algoritmos de clasificación para predecir la clase o categoría de un conjunto de elementos.\n",
    "\n",
    "Algoritmos de clasificación:\n",
    "\n",
    "    K-vecinos más cercanos\n",
    "    Árboles de decisión\n",
    "    Bosque aleatorio\n",
    "    Máquinas de vectores soporte\n",
    "    Bayes ingenuo\n",
    "    Redes Neuronales\n",
    "\n",
    "3. Agrupación\n",
    "\n",
    "Usamos algoritmos de agrupamiento para resumir o estructurar datos.\n",
    "\n",
    "Algoritmos de agrupamiento:\n",
    "\n",
    "    K-means\n",
    "    DBSCAN\n",
    "    Jerárquico\n",
    "\n",
    "4. Asociación\n",
    "\n",
    "Usamos algoritmos de asociación para asociar elementos o eventos concurrentes.\n",
    "\n",
    "Algoritmos de asociación:\n",
    "\n",
    "    A priori\n",
    "\n",
    "5. Detección de anomalías\n",
    "\n",
    "Usamos la detección de anomalías para descubrir actividades anormales y casos inusuales como la detección de fraudes.\n",
    "\n",
    "6. Minería de patrones de secuencia\n",
    "\n",
    "Usamos la minería de patrones secuenciales para predecir los próximos eventos de datos entre ejemplos de datos en una secuencia.\n",
    "\n",
    "7. Reducción de dimensionalidad\n",
    "\n",
    "Usamos la reducción de dimensionalidad para reducir el tamaño de los datos para extraer solo características útiles de un conjunto de datos.\n",
    "\n",
    "8. Sistemas de recomendación\n",
    "\n",
    "Usamos algoritmos de recomendación para construir motores de recomendación.\n",
    "\n",
    "Ejemplos:\n",
    "\n",
    "    Sistema de recomendación de Netflix.\n",
    "    Un sistema de recomendación de libros.\n",
    "    Un sistema de recomendación de productos en Amazon.\n",
    "\n",
    "Hoy en día, escuchamos muchas palabras de moda como inteligencia artificial, aprendizaje automático, aprendizaje profundo y otros...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cuáles son las diferencias fundamentales entre inteligencia artificial, aprendizaje automático y aprendizaje profundo?\n",
    "\n",
    "1. Inteligencia artificial (IA):\n",
    "\n",
    "La inteligencia artificial (IA), como la define el profesor Andrew Moore, es la ciencia y la ingeniería para hacer que las computadoras se comporten de una manera que, hasta hace poco, pensábamos que requería inteligencia humana.\n",
    "\n",
    "Éstos incluyen:\n",
    "\n",
    "    Visión por computador\n",
    "    Procesamiento de lenguaje\n",
    "    Creatividad\n",
    "    Resumen\n",
    "\n",
    "2. Aprendizaje automático (ML):\n",
    "\n",
    "Según lo definido por el profesor Tom Mitchell, el aprendizaje automático se refiere a una rama científica de la IA, que se centra en el estudio de algoritmos informáticos que permiten que los programas informáticos mejoren automáticamente a través de la experiencia.\n",
    "\n",
    "Éstos incluyen:\n",
    "\n",
    "    Clasificación\n",
    "    Red neuronal\n",
    "    Agrupación\n",
    "\n",
    "3. Aprendizaje profundo:\n",
    "\n",
    "El aprendizaje profundo es un subconjunto del aprendizaje automático en el que las redes neuronales en capas, combinadas con una alta potencia informática y grandes conjuntos de datos, pueden crear modelos de aprendizaje automático potentes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicaciones de las redes neuronales artificiales:\n",
    "\n",
    "1. Clasificación de datos:\n",
    "    * Basándose en un conjunto de datos, nuestra red neuronal de entrenada predice si es un perro o un gato.\n",
    "        \n",
    "\n",
    "2. Detección de anomalías:\n",
    "    * Dados los detalles sobre las transacciones de una persona, se puede decir si la transacción es un fraude o no.\n",
    "\n",
    "\n",
    "3. Reconocimiento de voz:\n",
    "    * Podemos entrenar nuestra red neuronal para reconocer patrones de habla. Ejemplo: Siri, Alexa, asistente de Google.\n",
    "\n",
    "\n",
    "4. Generación de audio:\n",
    "    * Dadas las entradas como archivos de audio, puede generar nueva música basada en varios factores como el género, el cantante y otros.\n",
    "\n",
    "\n",
    "5. Análisis de series temporales:\n",
    "    * Una red neuronal bien entrenada puede predecir el precio de las acciones.\n",
    "\n",
    "\n",
    "6. Corrección ortográfica:\n",
    "    * Podemos entrenar una red neuronal que detecta errores ortográficos y también puede sugerir un significado similar para las palabras.\n",
    "\n",
    "\n",
    "7. Reconocimiento de caracteres:\n",
    "    * Una red neuronal bien entrenada puede detectar caracteres escritos a mano.\n",
    "\n",
    "\n",
    "8. Máquina traductora:\n",
    "    * Podemos desarrollar una red neuronal que traduzca un idioma a otro.\n",
    "\n",
    "\n",
    "9. Procesamiento de imágenes:\n",
    "    * Podemos entrenar una red neuronal para procesar una imagen y extraer información de ella.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "\n",
    "## Bloques de construcción: neuronas\n",
    "\n",
    "Primero, tenemos que hablar de neuronas, la unidad básica de una red neuronal. Una neurona toma entradas, hace algunas operaciones matemáticas con ellas y produce una salida. Así es como se ve una neurona de 2 entradas:\n",
    "\n",
    "![img](https://victorzhou.com/a74a19dc0599aae11df7493c718abaf9/perceptron.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí están pasando 3 cosas. Primero, cada entrada se multiplica por un peso:\n",
    "$$x_1 → x_1 ∗ w_1$$ \n",
    "\n",
    "$$x_2 → x_2 ∗ w_2$$\n",
    "\n",
    "A continuación, todas las entradas ponderadas se suman junto con un sesgo b:\n",
    "\n",
    "$$(x1 ∗ w1) + (x2 ∗ w2) + b$$\n",
    "\n",
    "Finalmente, la suma se pasa a través de una función de activación:\n",
    "$$y = f (x1 ∗ w1 + x2 ∗ w2 + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de activación se utiliza para convertir una entrada ilimitada en una salida que tiene una forma agradable y predecible. Una función de activación comúnmente utilizada es la función sigmoidal:\n",
    "\n",
    "-------\n",
    "\n",
    "![img](https://www.researchgate.net/publication/259395938/figure/fig9/AS:341806387613697@1458504401241/Three-phases-of-growth-in-a-typical-sigmoidal-curve-A-sigmoidal-curve-solid-black-line.png)\n",
    "\n",
    "-------\n",
    "\n",
    "La función sigmoidal solo genera números en el rango (0,1). Puedes pensar en ello como comprimir (−∞, + ∞) a (0,1) - números grandes negativos se convierten en ~ 0 y los números positivos grandes se convierten en ~ 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un ejemplo simple\n",
    "\n",
    "Supongamos que tenemos una neurona de 2 entradas que usa la función de activación sigmoidea y tiene los siguientes parámetros:\n",
    "* w = [0,1] \n",
    "* b = 4\n",
    "\n",
    "Ahora, démosle a la neurona una entrada de x = [2,3]. Usaremos el producto escalar para escribir cosas de manera más concisa:\n",
    "\n",
    "$$(w⋅x) + b = ((w1 ∗ x1) + (w2 ∗ x2)) + b = 0 ∗ 2 + 1 ∗ 3 + 4 = 7 $$\n",
    "\n",
    "$$y = f (w⋅x + b) = f (7) = 0.999$$\n",
    "\n",
    "La neurona genera 0.999dadas las entradas x = [2,3]. Este proceso de pasar entradas hacia adelante para obtener una salida se conoce como feedforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990889488055994\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  # Our activation function: f(x) = 1 / (1 + e^(-x))\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "  def __init__(self, weights, bias):\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "\n",
    "  def feedforward(self, inputs):\n",
    "    # Weight inputs, add bias, then use the activation function\n",
    "    total = np.dot(self.weights, inputs) + self.bias\n",
    "    return sigmoid(total)\n",
    "\n",
    "weights = np.array([0, 1]) # w1 = 0, w2 = 1\n",
    "bias = 4                   # b = 4\n",
    "n = Neuron(weights, bias)\n",
    "\n",
    "x = np.array([2, 3])       # x1 = 2, x2 = 3\n",
    "print(n.feedforward(x))    # 0.9990889488055994"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinando neuronas en una red neuronal\n",
    "\n",
    "Una red neuronal no es más que un grupo de neuronas conectadas entre sí. Así es como se vería una red neuronal simple:\n",
    "\n",
    "![img](https://victorzhou.com/77ed172fdef54ca1ffcfb0bba27ba334/network.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usemos la red que se muestra arriba y supongamos que todas las neuronas tienen los mismos pesos w = [0,1], el mismo sesgo b = 0, y el mismo función de activación sigmoidal. Dejemos que h1, h2, o1 denoten las salidas de las neuronas que representan.\n",
    "\n",
    "¿Qué sucede si pasamos la entrada x = [2,3]?\n",
    "\n",
    "$$h1 = h2 = f (w⋅x + b) = f ((0 ∗ 2) + (1 ∗ 3) +0) = f (3) = 0.9526$$\n",
    "\n",
    "$$o1 = f (w⋅ [h1, h2] + b) = f ((0 ∗ h1) + (1 ∗ h2) +0) = f (0.9526) = 0.7216$$\n",
    "\n",
    "La salida de la red neuronal para la entrada x = [2,3] es 0.7216. Bastante simple, ¿verdad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una red neuronal puede tener cualquier cantidad de capas con cualquier cantidad de neuronas en esas capas. La idea básica sigue siendo la misma: alimente la (s) entrada (s) hacia adelante a través de las neuronas en la red para obtener la (s) salida (s) al final. Para simplificar, seguiremos usando la red que se muestra arriba durante el resto de esta publicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.350\n",
      "Epoch 10 loss: 0.238\n",
      "Epoch 20 loss: 0.143\n",
      "Epoch 30 loss: 0.095\n",
      "Epoch 40 loss: 0.069\n",
      "Epoch 50 loss: 0.054\n",
      "Epoch 60 loss: 0.043\n",
      "Epoch 70 loss: 0.036\n",
      "Epoch 80 loss: 0.030\n",
      "Epoch 90 loss: 0.026\n",
      "Epoch 100 loss: 0.023\n",
      "Epoch 110 loss: 0.020\n",
      "Epoch 120 loss: 0.018\n",
      "Epoch 130 loss: 0.016\n",
      "Epoch 140 loss: 0.015\n",
      "Epoch 150 loss: 0.014\n",
      "Epoch 160 loss: 0.013\n",
      "Epoch 170 loss: 0.012\n",
      "Epoch 180 loss: 0.011\n",
      "Epoch 190 loss: 0.010\n",
      "Epoch 200 loss: 0.010\n",
      "Epoch 210 loss: 0.009\n",
      "Epoch 220 loss: 0.009\n",
      "Epoch 230 loss: 0.008\n",
      "Epoch 240 loss: 0.008\n",
      "Epoch 250 loss: 0.007\n",
      "Epoch 260 loss: 0.007\n",
      "Epoch 270 loss: 0.007\n",
      "Epoch 280 loss: 0.006\n",
      "Epoch 290 loss: 0.006\n",
      "Epoch 300 loss: 0.006\n",
      "Epoch 310 loss: 0.006\n",
      "Epoch 320 loss: 0.006\n",
      "Epoch 330 loss: 0.005\n",
      "Epoch 340 loss: 0.005\n",
      "Epoch 350 loss: 0.005\n",
      "Epoch 360 loss: 0.005\n",
      "Epoch 370 loss: 0.005\n",
      "Epoch 380 loss: 0.005\n",
      "Epoch 390 loss: 0.004\n",
      "Epoch 400 loss: 0.004\n",
      "Epoch 410 loss: 0.004\n",
      "Epoch 420 loss: 0.004\n",
      "Epoch 430 loss: 0.004\n",
      "Epoch 440 loss: 0.004\n",
      "Epoch 450 loss: 0.004\n",
      "Epoch 460 loss: 0.004\n",
      "Epoch 470 loss: 0.004\n",
      "Epoch 480 loss: 0.003\n",
      "Epoch 490 loss: 0.003\n",
      "Epoch 500 loss: 0.003\n",
      "Epoch 510 loss: 0.003\n",
      "Epoch 520 loss: 0.003\n",
      "Epoch 530 loss: 0.003\n",
      "Epoch 540 loss: 0.003\n",
      "Epoch 550 loss: 0.003\n",
      "Epoch 560 loss: 0.003\n",
      "Epoch 570 loss: 0.003\n",
      "Epoch 580 loss: 0.003\n",
      "Epoch 590 loss: 0.003\n",
      "Epoch 600 loss: 0.003\n",
      "Epoch 610 loss: 0.003\n",
      "Epoch 620 loss: 0.003\n",
      "Epoch 630 loss: 0.003\n",
      "Epoch 640 loss: 0.002\n",
      "Epoch 650 loss: 0.002\n",
      "Epoch 660 loss: 0.002\n",
      "Epoch 670 loss: 0.002\n",
      "Epoch 680 loss: 0.002\n",
      "Epoch 690 loss: 0.002\n",
      "Epoch 700 loss: 0.002\n",
      "Epoch 710 loss: 0.002\n",
      "Epoch 720 loss: 0.002\n",
      "Epoch 730 loss: 0.002\n",
      "Epoch 740 loss: 0.002\n",
      "Epoch 750 loss: 0.002\n",
      "Epoch 760 loss: 0.002\n",
      "Epoch 770 loss: 0.002\n",
      "Epoch 780 loss: 0.002\n",
      "Epoch 790 loss: 0.002\n",
      "Epoch 800 loss: 0.002\n",
      "Epoch 810 loss: 0.002\n",
      "Epoch 820 loss: 0.002\n",
      "Epoch 830 loss: 0.002\n",
      "Epoch 840 loss: 0.002\n",
      "Epoch 850 loss: 0.002\n",
      "Epoch 860 loss: 0.002\n",
      "Epoch 870 loss: 0.002\n",
      "Epoch 880 loss: 0.002\n",
      "Epoch 890 loss: 0.002\n",
      "Epoch 900 loss: 0.002\n",
      "Epoch 910 loss: 0.002\n",
      "Epoch 920 loss: 0.002\n",
      "Epoch 930 loss: 0.002\n",
      "Epoch 940 loss: 0.002\n",
      "Epoch 950 loss: 0.002\n",
      "Epoch 960 loss: 0.002\n",
      "Epoch 970 loss: 0.002\n",
      "Epoch 980 loss: 0.002\n",
      "Epoch 990 loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
    "  fx = sigmoid(x)\n",
    "  return fx * (1 - fx)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "  # y_true and y_pred are numpy arrays of the same length.\n",
    "  return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "  '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "\n",
    "  *** DISCLAIMER ***:\n",
    "  The code below is intended to be simple and educational, NOT optimal.\n",
    "  Real neural net code looks nothing like this. DO NOT use this code.\n",
    "  Instead, read/run it to understand how this specific network works.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    # Weights\n",
    "    self.w1 = np.random.normal()\n",
    "    self.w2 = np.random.normal()\n",
    "    self.w3 = np.random.normal()\n",
    "    self.w4 = np.random.normal()\n",
    "    self.w5 = np.random.normal()\n",
    "    self.w6 = np.random.normal()\n",
    "\n",
    "    # Biases\n",
    "    self.b1 = np.random.normal()\n",
    "    self.b2 = np.random.normal()\n",
    "    self.b3 = np.random.normal()\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    # x is a numpy array with 2 elements.\n",
    "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "    return o1\n",
    "\n",
    "  def train(self, data, all_y_trues):\n",
    "    '''\n",
    "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "    - all_y_trues is a numpy array with n elements.\n",
    "      Elements in all_y_trues correspond to those in data.\n",
    "    '''\n",
    "    learn_rate = 0.1\n",
    "    epochs = 1000 # number of times to loop through the entire dataset\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for x, y_true in zip(data, all_y_trues):\n",
    "        # --- Do a feedforward (we'll need these values later)\n",
    "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "        h1 = sigmoid(sum_h1)\n",
    "\n",
    "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "        h2 = sigmoid(sum_h2)\n",
    "\n",
    "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "        o1 = sigmoid(sum_o1)\n",
    "        y_pred = o1\n",
    "\n",
    "        # --- Calculate partial derivatives.\n",
    "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "        # Neuron o1\n",
    "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "        # Neuron h1\n",
    "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "        # Neuron h2\n",
    "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "        # --- Update weights and biases\n",
    "        # Neuron h1\n",
    "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "        # Neuron h2\n",
    "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "        # Neuron o1\n",
    "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "      # --- Calculate total loss at the end of each epoch\n",
    "      if epoch % 10 == 0:\n",
    "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "        loss = mse_loss(all_y_trues, y_preds)\n",
    "        print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "\n",
    "# Define dataset\n",
    "data = np.array([\n",
    "  [-2, -1],  # Alice\n",
    "  [25, 6],   # Bob\n",
    "  [17, 4],   # Charlie\n",
    "  [-15, -6], # Diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "  1, # Alice\n",
    "  0, # Bob\n",
    "  0, # Charlie\n",
    "  1, # Diana\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://victorzhou.com/static/99e7886af56d6f41b484d17a52f9241b/3e495/loss.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily: 0.965\n",
      "Frank: 0.039\n"
     ]
    }
   ],
   "source": [
    "# Make some predictions\n",
    "emily = np.array([-7, -3]) # 128 pounds, 63 inches\n",
    "frank = np.array([20, 2])  # 155 pounds, 68 inches\n",
    "print(\"Emily: %.3f\" % network.feedforward(emily)) # 0.951 - F\n",
    "print(\"Frank: %.3f\" % network.feedforward(frank)) # 0.039 - M"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
